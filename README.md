# ğŸ¤– RoboBack
<img src="./examples/RoboBack_example.png" width="600" height="420">

<img src="./examples/compressed.gif" width="600" height="420">

**RoboBack** is a command-line tool that allows you to **travel back in time** and extract historical `robots.txt` files of a target domain from [archive.org (Wayback Machine)](https://web.archive.org).

It can be useful for OSINT, bug bounty hunting, and understanding how a target's crawling policy evolved over time.

---

## ğŸ“¦ Features

- Fetch historical `robots.txt` files from Wayback Machine
- Filter results by year(s)
- Save the extracted content to a file
- Colored output for better readability
- Clean and easy-to-use CLI interface

---

## ğŸ§° Usage

```bash
python3 RoboBack.py -d <domain> [options]
```

### ğŸ“Œ Required Argument

- `-d <domain>`  
  The domain you want to scan (e.g., `example.com`)

### âš™ï¸ Optional Arguments

- `-o <file>`  
  Save the output to a file.

- `-tf <years>`  
  Filter by specific years (comma-separated), e.g., `2021,2022`

## ğŸ“¥ Installation

```bash
git clone https://github.com/yourusername/RoboBack.git
cd RoboBack
pip install -r requirements.txt
```
  
## ğŸ§  Why Use RoboBack?

- Understand which parts of a site were previously disallowed
- Look for hidden directories or endpoints no longer in use
- Trace the history of domain structures and crawling behavior

---
**created by:** nakutenshi

Ø¨Ù‡ Ø®Ø¯Ø§ Ù†Ù…ÛŒØ¯ÙˆÙ†Ù… Ø¯Ø§Ø±Ù… Ø¨Ø§ Ø²Ù†Ø¯Ú¯ÛŒÙ… Ú†ÛŒ Ú©Ø§Ø± Ù…ÛŒÚ©Ù†Ù…

Ø­Ø§Ù„Ù… Ø§Ø² Ø§ÛŒÙ† Ø§Ø¯Ù…Ø§ Ø¨Ù‡Ù… Ù…ÛŒØ®ÙˆØ±Ù‡

Ø·Ø±Ø² Ø­Ø±Ù Ø²Ø¯Ù†Ø´ÙˆÙ† 

Ø·Ø±Ø² ØªÙÚ©Ø±Ø´ÙˆÙ†

Ø·Ø±Ø² Ù‚Ø¶Ø§ÙˆØª Ú©Ø±Ø¯Ù†Ø´ÙˆÙ†

Ø·Ø±Ø² Ø±ÙØªØ§Ø± Ú©Ø±Ø¯Ù†Ø´ÙˆÙ†

Ø±Ø§Ø³ØªØ´ÙˆÙ† Ø¨Ø®ÙˆØ§ÛŒ ÙÚ©Ø± Ù…ÛŒÚ©Ø±Ø¯Ù… Ù‡Ù…Ù‡ Ø§Ø¯Ù…Ùˆ Ø§ÛŒÙ†Ø¬ÙˆØ±Ù†

ÙˆÙ„ÛŒ ÛŒÙ‡ Ø¯Ø³ØªÙ‡ Ø®Ø§ØµÛŒØ´ÙˆÙ† ÙØ±Ù‚ Ø¯Ø§Ø´ØªÙ†

ÙˆÙ„ÛŒ Ù…Ù† Ø¬Ø²Ùˆ Ø¨Ù‡ Ø§ÙˆÙ† Ø¯Ø³ØªÙ‡ Ù†Ù…ÛŒØ´Ø¯Ù…

Ù…ÛŒØ®ÙˆØ§Ø³ØªÙ… Ù‡Ù… Ù†Ù…ÛŒØªÙˆÙ†Ø³ØªÙ…
