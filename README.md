# ğŸ¤– RoboBack

**RoboBack** is a command-line tool that allows you to **travel back in time** and extract historical `robots.txt` files of a target domain from [archive.org (Wayback Machine)](https://web.archive.org).

It can be useful for OSINT, bug bounty hunting, and understanding how a target's crawling policy evolved over time.

---

## ğŸ“¦ Features

- Fetch historical `robots.txt` files from Wayback Machine
- Filter results by year(s)
- Save the extracted content to a file
- Colored output for better readability
- Clean and easy-to-use CLI interface

---

## ğŸ§° Usage

```bash
python3 RoboBack.py -d <domain> [options]
```

### ğŸ“Œ Required Argument

- `-d <domain>`  
  The domain you want to scan (e.g., `example.com`)

### âš™ï¸ Optional Arguments

- `-o <file>`  
  Save the output to a file.

- `-tf <years>`  
  Filter by specific years (comma-separated), e.g., `2021,2022`

## ğŸ“¥ Installation

```bash
git clone https://github.com/yourusername/RoboBack.git
cd RoboBack
pip install -r requirements.txt
```
  
## ğŸ§  Why Use RoboBack?

- Understand which parts of a site were previously disallowed

- Look for hidden directories or endpoints no longer in use

- Trace the history of domain structures and crawling behavior

---
**created by:** nakutenshi
